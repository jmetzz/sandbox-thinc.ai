{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: thinc>=8.0.0a0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (8.0.0a1)\n",
      "Requirement already satisfied: ml_datasets in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (0.1.6)\n",
      "Requirement already satisfied: tqdm>=4.41 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (4.46.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc>=8.0.0a0) (2.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc>=8.0.0a0) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc>=8.0.0a0) (1.18.1)\n",
      "Requirement already satisfied: pydantic<2.0.0,>=1.4.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc>=8.0.0a0) (1.5.1)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4.1; python_version < \"3.8\" in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc>=8.0.0a0) (3.7.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc>=8.0.0a0) (0.6.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.0.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc>=8.0.0a0) (2.0.1)\n",
      "Requirement already satisfied: catalogue<3.0.0,>=0.2.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc>=8.0.0a0) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc>=8.0.0a0) (47.1.1.post20200604)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc>=8.0.0a0) (1.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from catalogue<3.0.0,>=0.2.0->thinc>=8.0.0a0) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<3.0.0,>=0.2.0->thinc>=8.0.0a0) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"thinc>=8.0.0a0\" ml_datasets \"tqdm>=4.41\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thinc.api import prefer_gpu\n",
    "prefer_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ml_datasets\n",
    "from tqdm import tqdm\n",
    "from thinc.api import fix_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fix_random_seed(0)\n",
    "\n",
    "def train_model(data, model, optimizer, n_iter, batch_size):\n",
    "    (train_X, train_y), (test_X, test_y) = data\n",
    "    model.initialize(X=train_X[:5], Y=train_y[:5])\n",
    "    for n in range(n_iter):\n",
    "        loss = 0.0\n",
    "        batches = model.ops.multibatch(batch_size, train_X, train_y, shuffle=True)\n",
    "        for X, Y in tqdm(batches, leave=False):\n",
    "            Yh, backprop = model.begin_update(X)\n",
    "            d_loss = []\n",
    "            for i in range(len(Yh)):\n",
    "                d_loss.append(Yh[i] - Y[i])\n",
    "                loss += ((Yh[i] - Y[i]) ** 2).sum()\n",
    "            backprop(d_loss)\n",
    "            model.finish_update(optimizer)\n",
    "        score = evaluate(model, test_X, test_y, batch_size)\n",
    "        print(f\"{n}\\t{loss:.2f}\\t{score:.3f}\")\n",
    "        \n",
    "def evaluate(model, test_X, test_Y, batch_size):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X, Y in model.ops.multibatch(batch_size, test_X, test_Y):\n",
    "        Yh = model.predict(X)\n",
    "        for yh, y in zip(Yh, Y):\n",
    "            correct += (y.argmax(axis=1) == yh.argmax(axis=1)).sum()\n",
    "            total += y.shape[0]\n",
    "    return float(correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Composing the model in code\n",
    "\n",
    "Here's the model definition, using the `>>` operator for the `chain` combinator.\n",
    "The `strings2arrays` transform converts a sequence of strings to a list of arrays. `with_array` \n",
    "transforms sequences (the sequences of arrays) into a contiguous 2-dimensional array on the way into \n",
    "and out of the model it wraps. This means our model has the following signature: \n",
    "`Model[Sequence[str], Sequence[Array2d]]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "from thinc.api import Model, chain, strings2arrays, with_array, HashEmbed, expand_window, Relu, Softmax, Adam, warmup_linear\n",
    "\n",
    "width = 32\n",
    "vector_width = 16\n",
    "nr_classes = 17\n",
    "learn_rate = 0.001\n",
    "n_iter = 10\n",
    "batch_size = 128\n",
    "\n",
    "with Model.define_operators({\">>\": chain}):\n",
    "    model = strings2arrays() >> with_array(\n",
    "        HashEmbed(nO=width, nV=vector_width, column=0)\n",
    "        >> expand_window(window_size=1)\n",
    "        >> Relu(nO=width, nI=width * 3)\n",
    "        >> Relu(nO=width, nI=width)\n",
    "        >> Softmax(nO=nr_classes, nI=width)\n",
    "    )\n",
    "optimizer = Adam(learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing the model via config file\n",
    "\n",
    "If we want to rebuild the model defined above in a config file, we first need to break down its structure:\n",
    "\n",
    "* `chain` (any number of positional arguments)\n",
    "  * `strings2arrays` (no arguments)\n",
    "  * `with_array` (one argument **layer**)\n",
    "    * **layer:** `chain` (any number of positional arguments)\n",
    "      * `HashEmbed`\n",
    "      * `expand_window`\n",
    "      * `Relu`\n",
    "      * `Relu`\n",
    "      * `Softmax`\n",
    "\n",
    "`chain` takes a variable number of positional arguments (the layers to compose). \n",
    "In the config, positional arguments can be expressed using `*` in the dot notation. For example, \n",
    "`model.layer` could describe a function passed to `model` as the argument `layer`, while `model.*.relu` \n",
    "defines a positional argument passed to `model`. The name of the argument, e.g. `relu` – doesn't matter \n",
    "in this case. It just needs to be unique.\n",
    "\n",
    "> ⚠️ **Important note:** It is recommended to use a hybrid approach: wrap the model definition in a registered function\n",
    "> and configure it via the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'index': 0,\n",
       "   'name': 'strings2arrays>>with_array-ints-getitem>>hashembed>>expand_window>>relu>>relu>>softmax',\n",
       "   'dims': {'nO': None, 'nI': None},\n",
       "   'refs': {}},\n",
       "  {'index': 1, 'name': 'strings2arrays', 'dims': {}, 'refs': {}},\n",
       "  {'index': 2,\n",
       "   'name': 'with_array-ints-getitem>>hashembed>>expand_window>>relu>>relu>>softmax',\n",
       "   'dims': {},\n",
       "   'refs': {}},\n",
       "  {'index': 3,\n",
       "   'name': 'ints-getitem>>hashembed>>expand_window>>relu>>relu>>softmax',\n",
       "   'dims': {'nO': None, 'nI': None},\n",
       "   'refs': {}},\n",
       "  {'index': 4,\n",
       "   'name': 'ints-getitem>>hashembed',\n",
       "   'dims': {'nO': None, 'nI': None},\n",
       "   'refs': {}},\n",
       "  {'index': 5, 'name': 'expand_window', 'dims': {}, 'refs': {}},\n",
       "  {'index': 6, 'name': 'relu', 'dims': {'nO': 32, 'nI': 96}, 'refs': {}},\n",
       "  {'index': 7, 'name': 'relu', 'dims': {'nO': 32, 'nI': 32}, 'refs': {}},\n",
       "  {'index': 8, 'name': 'softmax', 'dims': {'nO': 17, 'nI': 32}, 'refs': {}},\n",
       "  {'index': 9, 'name': 'ints-getitem', 'dims': {}, 'refs': {}},\n",
       "  {'index': 10,\n",
       "   'name': 'hashembed',\n",
       "   'dims': {'nO': 32, 'nV': 16, 'nI': None},\n",
       "   'refs': {}}],\n",
       " 'attrs': [{},\n",
       "  {},\n",
       "  {'pad': b'\\x00'},\n",
       "  {},\n",
       "  {'column': b'\\x00'},\n",
       "  {'window_size': b'\\x01'},\n",
       "  {},\n",
       "  {},\n",
       "  {},\n",
       "  {},\n",
       "  {'column': b'\\x00', 'seed': b'\\xcc\\xd2'}],\n",
       " 'params': [{},\n",
       "  {},\n",
       "  {},\n",
       "  {},\n",
       "  {},\n",
       "  {},\n",
       "  {'W': None, 'b': None},\n",
       "  {'W': None, 'b': None},\n",
       "  {'W': None, 'b': None},\n",
       "  {},\n",
       "  {'E': None}],\n",
       " 'shims': [[], [], [], [], [], [], [], [], [], [], []]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG = \"\"\"\n",
    "[hyper_params]\n",
    "width = 32\n",
    "vector_width = 16\n",
    "learn_rate = 0.001\n",
    "n_tags = 17\n",
    "\n",
    "[training]\n",
    "n_iter = 10\n",
    "batch_size = 128\n",
    "\n",
    "[model]\n",
    "@layers = \"chain.v1\"\n",
    "\n",
    "[model.*.strings2arrays]\n",
    "@layers = \"strings2arrays.v1\"\n",
    "\n",
    "[model.*.with_array]\n",
    "@layers = \"with_array.v1\"\n",
    "\n",
    "[model.*.with_array.layer]\n",
    "@layers = \"chain.v1\"\n",
    "\n",
    "[model.*.with_array.layer.*.hashembed]\n",
    "@layers = \"HashEmbed.v1\"\n",
    "nO = ${hyper_params:width}\n",
    "nV = ${hyper_params:vector_width}\n",
    "column = 0\n",
    "\n",
    "[model.*.with_array.layer.*.expand_window]\n",
    "@layers = \"expand_window.v1\"\n",
    "window_size = 1\n",
    "\n",
    "[model.*.with_array.layer.*.relu1]\n",
    "@layers = \"Relu.v1\"\n",
    "nO = ${hyper_params:width}\n",
    "nI = 96\n",
    "\n",
    "[model.*.with_array.layer.*.relu2]\n",
    "@layers = \"Relu.v1\"\n",
    "nO = ${hyper_params:width}\n",
    "nI = ${hyper_params:width}\n",
    "\n",
    "[model.*.with_array.layer.*.softmax]\n",
    "@layers = \"Softmax.v1\"\n",
    "nO = ${hyper_params:n_tags}\n",
    "nI = ${hyper_params:width}\n",
    "\n",
    "[optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "learn_rate = ${hyper_params:learn_rate}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from thinc.api import registry, Config\n",
    "\n",
    "config = Config().from_str(CONFIG)\n",
    "loaded_config = registry.make_from_config(config)\n",
    "\n",
    "model = loaded_config[\"model\"]\n",
    "optimizer = loaded_config[\"optimizer\"]\n",
    "n_iter = loaded_config[\"training\"][\"n_iter\"]\n",
    "batch_size = loaded_config[\"training\"][\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 13/112 [00:00<00:00, 124.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t394039.86\t0.427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 14/112 [00:00<00:00, 132.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t293592.17\t0.530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 13/112 [00:00<00:00, 126.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\t257757.83\t0.573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 13/112 [00:00<00:00, 120.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t234177.70\t0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 13/112 [00:00<00:00, 124.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\t212988.53\t0.640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 12/112 [00:00<00:00, 116.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\t201683.23\t0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 13/112 [00:00<00:00, 118.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\t194722.60\t0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 12/112 [00:00<00:00, 117.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\t189556.96\t0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 12/112 [00:00<00:00, 117.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\t184881.38\t0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\t180871.01\t0.691\n"
     ]
    }
   ],
   "source": [
    "data = ml_datasets.ud_ancora_pos_tags()\n",
    "train_model(data, model, optimizer, n_iter, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing the model with code and config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbb71ce08364d5f86f58164973c4212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t392620.84\t0.403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b91df8fa314733a9164255a25bcb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t298324.66\t0.526\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef85921e47c41c5a4baa0ca25f27cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\t260850.97\t0.579\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15dcf0a4e8a4ff699fc3d366e3ba58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t240399.34\t0.606\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630f5fa9940b49c7b7a2edfdaee1ed15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\t225486.50\t0.629\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f50da23f584238bdc990989775c6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\t214070.91\t0.645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7af4558d60423cad10de51d6d28db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\t205949.52\t0.656\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a24bfd1f0bd446ca724c51e35efc067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\t199703.57\t0.665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf23981353f417f874bbdfd2165c9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\t194561.25\t0.674\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a788fcb8d0ac4ac39f18096cbb6ab08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\t189968.34\t0.681\n"
     ]
    }
   ],
   "source": [
    "import thinc\n",
    "from thinc.api import Model, chain, strings2arrays, with_array, HashEmbed, expand_window, Relu, Softmax,\\\n",
    "    Adam, warmup_linear\n",
    "\n",
    "@thinc.registry.layers(\"cnn_tagger.v1\")\n",
    "def create_cnn_tagger(width: int, vector_width: int, nr_classes: int = 17) -> Model:\n",
    "    with Model.define_operators({\">>\": chain}):\n",
    "        model = strings2arrays() >> with_array(\n",
    "            HashEmbed(nO=width, nV=vector_width, column=0)\n",
    "            >> expand_window(window_size=1)\n",
    "            >> Relu(nO=width, nI=width * 3)\n",
    "            >> Relu(nO=width, nI=width)\n",
    "            >> Softmax(nO=nr_classes, nI=width)\n",
    "        )\n",
    "    return model\n",
    "\n",
    "CONFIG = \"\"\"\n",
    "[hyper_params]\n",
    "width = 32\n",
    "vector_width = 16\n",
    "learn_rate = 0.001\n",
    "\n",
    "[training]\n",
    "n_iter = 10\n",
    "batch_size = 128\n",
    "\n",
    "[model]\n",
    "@layers = \"cnn_tagger.v1\"\n",
    "width = ${hyper_params:width}\n",
    "vector_width = ${hyper_params:vector_width}\n",
    "nr_classes = 17\n",
    "\n",
    "[optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "learn_rate = ${hyper_params:learn_rate}\n",
    "\"\"\"\n",
    "\n",
    "loaded_config = registry.make_from_config(Config().from_str(CONFIG))\n",
    "\n",
    "model = loaded_config[\"model\"]\n",
    "optimizer = loaded_config[\"optimizer\"]\n",
    "n_iter = loaded_config[\"training\"][\"n_iter\"]\n",
    "batch_size = loaded_config[\"training\"][\"batch_size\"]\n",
    "\n",
    "data = ml_datasets.ud_ancora_pos_tags()\n",
    "\n",
    "train_model(data, model, optimizer, n_iter, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}