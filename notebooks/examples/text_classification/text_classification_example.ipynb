{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: thinc in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (8.0.0a1)\nCollecting syntok\n  Downloading syntok-1.3.1.tar.gz (23 kB)\nRequirement already satisfied: ml_datasets in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (0.1.6)\nRequirement already satisfied: tqdm in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (4.46.0)\nRequirement already satisfied: catalogue<3.0.0,>=0.2.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc) (2.0.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc) (1.0.2)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc) (0.6.0)\nRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4.1; python_version < \"3.8\" in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc) (3.7.4.1)\nRequirement already satisfied: numpy>=1.7.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc) (1.18.1)\nRequirement already satisfied: setuptools in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc) (47.1.1.post20200604)\nRequirement already satisfied: pydantic<2.0.0,>=1.4.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc) (1.5.1)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc) (2.0.3)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc) (3.0.2)\nRequirement already satisfied: srsly<3.0.0,>=2.0.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from thinc) (2.0.1)\nCollecting regex\n  Downloading regex-2020.6.8.tar.gz (690 kB)\n\u001b[K     |████████████████████████████████| 690 kB 2.6 MB/s \n\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from catalogue<3.0.0,>=0.2.0->thinc) (1.6.0)\nRequirement already satisfied: zipp>=0.5 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<3.0.0,>=0.2.0->thinc) (3.1.0)\nBuilding wheels for collected packages: syntok, regex\n  Building wheel for syntok (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for syntok: filename=syntok-1.3.1-py3-none-any.whl size=20916 sha256=3863d3dfc33f9bd46b4c3a41c5be810617349012ba96235a0e302131b43d940e\n  Stored in directory: /Users/jean.metz/Library/Caches/pip/wheels/5e/c2/33/e5d7d8f2f8b0c391d76bf82b844c3151bf23a84d75d02b185f\n  Building wheel for regex (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for regex: filename=regex-2020.6.8-cp37-cp37m-macosx_10_9_x86_64.whl size=283191 sha256=cd462451177171fb9e0f8822fd2330acf274b743b8c844b5cdf2cd5ec938af23\n  Stored in directory: /Users/jean.metz/Library/Caches/pip/wheels/46/f1/0b/a372e98f7103934a3573301c71b475143baf8ba6f6dffc876c\nSuccessfully built syntok regex\nInstalling collected packages: regex, syntok\nSuccessfully installed regex-2020.6.8 syntok-1.3.1\n"
    }
   ],
   "source": [
    "! pip install \"thinc>=8.0.0a0\" ml_datasets \"tqdm>=4.41\" syntok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.api import HashEmbed, Maxout, Softmax, expand_window, Relu\n",
    "from thinc.api import residual, strings2arrays, with_array, clone, chain, concatenate\n",
    "from thinc.layers import noop\n",
    "from syntok.tokenizer import Tokenizer\n",
    "\n",
    "import ml_datasets\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts):\n",
    "    tok = Tokenizer()\n",
    "    return [[token.value for token in tok.tokenize(text)] for text in texts]\n",
    "\n",
    "\n",
    "def load_data(limit:int = 1000):\n",
    "    (train_texts, train_cats), (dev_texts, dev_cats) = ml_datasets.dbpedia(limit=limit)\n",
    "    # train_texts, train_cats = zip(*train_data)\n",
    "    # dev_texts, dev_cats = zip(*dev_data)\n",
    "    unique_cats = list(numpy.unique(numpy.concatenate((train_cats, dev_cats))))\n",
    "    nr_class = len(unique_cats)\n",
    "    print(f\"{len(train_cats)} training / {len(dev_cats)} test\")\n",
    "    print(f\"{nr_class} classes\")\n",
    "\n",
    "    train_y = numpy.zeros((len(train_cats), nr_class), dtype=\"f\")\n",
    "    for i, cat in enumerate(train_cats):\n",
    "        train_y[i][unique_cats.index(cat)] = 1\n",
    "    dev_y = numpy.zeros((len(dev_cats), nr_class), dtype=\"f\")\n",
    "    for i, cat in enumerate(dev_cats):\n",
    "        dev_y[i][unique_cats.index(cat)] = 1\n",
    "\n",
    "    train_tokenized = tokenize_texts(train_texts)\n",
    "    dev_tokenized = tokenize_texts(dev_texts)\n",
    "    # Generate simple vocab mapping, <unk> is 0\n",
    "    vocab = {}\n",
    "    count_id = 1\n",
    "    for text in train_tokenized:\n",
    "        for token in text:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = count_id\n",
    "                count_id += 1\n",
    "    # Map texts using vocab\n",
    "    train_X = []\n",
    "    for text in train_tokenized:\n",
    "        train_X.append(numpy.array([vocab.get(t, 0) for t in text]))\n",
    "    dev_X = []\n",
    "    for text in dev_tokenized:\n",
    "        dev_X.append(numpy.array([vocab.get(t, 0) for t in text]))\n",
    "    return (train_X, train_y), (dev_X, dev_y), vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.api import add, chain, concatenate, clone\n",
    "from thinc.api import with_array, reduce_max, reduce_mean, residual\n",
    "from thinc.api import Model, Embed, Maxout, Softmax, Dropout, Relu\n",
    "\n",
    "nH = 5\n",
    "\n",
    "\n",
    "with Model.define_operators({\">>\": chain, \"|\": concatenate, \"+\": add, \"**\": clone}):\n",
    "    model = (\n",
    "        with_array(\n",
    "            (Embed(128, column=0) + Embed(64, column=1))\n",
    "            >> Maxout(nH, normalize=True, dropout=0.2)\n",
    "        )\n",
    "        >> (reduce_max() | reduce_mean())\n",
    "        >> residual(Relu() >> Dropout(0.2)) ** 2\n",
    "        >> Softmax()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "68345856it [00:01, 36810592.28it/s]\nUntaring file...\n2000 training / 2000 dev\n14 classes\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-066015cc6a4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'C' is not defined"
     ]
    }
   ],
   "source": [
    "(train_X, train_y), (dev_X, dev_y), vocab = load_data()\n",
    "\n",
    "batch_size = C[\"training\"][\"batch_size\"]\n",
    "optimizer = C[\"optimizer\"]\n",
    "model = C[\"model\"]\n",
    "model.get_ref(\"embed\").set_dim(\"nV\", len(vocab))\n",
    "\n",
    "model.initialize(X=train_X, Y=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dev_X, dev_Y, batch_size):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for X, Y in model.ops.multibatch(batch_size, dev_X, dev_Y):\n",
    "        Yh = model.predict(X)\n",
    "        for j in range(len(Yh)):\n",
    "            correct += Yh[j].argmax(axis=0) == Y[j].argmax(axis=0)\n",
    "        total += len(Y)\n",
    "    return float(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.api import fix_random_seed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "fix_random_seed(0)\n",
    "for n in range(C[\"training\"][\"n_iter\"]):\n",
    "    loss = 0.0\n",
    "    batches = model.ops.multibatch(batch_size, train_X, train_y, shuffle=True)\n",
    "    for X, Y in tqdm(batches, leave=False):\n",
    "        Yh, backprop = model.begin_update(X)\n",
    "        d_loss = []\n",
    "        for i in range(len(Yh)):\n",
    "            d_loss.append(Yh[i] - Y[i])\n",
    "            loss += ((Yh[i] - Y[i]) ** 2).sum()\n",
    "        backprop(numpy.array(d_loss))\n",
    "        model.finish_update(optimizer)\n",
    "    score = evaluate_model(model, dev_X, dev_y, batch_size)\n",
    "    print(f\"{n}\\t{loss:.2f}\\t{score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}