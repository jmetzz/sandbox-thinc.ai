{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Thinc Framework\n",
    "\n",
    "This example shows how to get started with Thinc, using the \"hello world\" of neural network models: recognizing handwritten digits from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "\n",
    "This notebook covers:\n",
    "- training the model\n",
    "- using config files\n",
    "- registering custom functions, and \n",
    "- wrapping models defined TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml_datasets in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (0.1.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from ml_datasets) (4.46.0)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from ml_datasets) (1.18.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=1.0.1 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from ml_datasets) (2.0.1)\n",
      "Requirement already satisfied: catalogue<3.0.0,>=0.2.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from ml_datasets) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from catalogue<3.0.0,>=0.2.0->ml_datasets) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<3.0.0,>=0.2.0->ml_datasets) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ml_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm>=4.41 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (4.46.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"tqdm>=4.41\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`prefer_gpu` function make sure we're performing operations on GPU if it is available. \n",
    "\n",
    "The function should be called right after importing Thinc, and it returns a boolean indicating whether the GPU has been activated.\n",
    "\n",
    "If GPU is not available or not properly setup, the code will still work, just slower :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thinc.api import prefer_gpu\n",
    "prefer_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset\n",
    "\n",
    "The `ml_datasets` package already packs MNIST dataset properly formated and splitted in training and test tuples.\n",
    "Thus, we only have to retrieve it and store in variables to be used.\n",
    "\n",
    "Each tuple is comprised of **features** and **target** elements, both are numpy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size=54000, dev size=10000\n"
     ]
    }
   ],
   "source": [
    "import ml_datasets\n",
    "data = (train_X, train_Y), (dev_X, dev_Y) = ml_datasets.mnist()\n",
    "print(f\"Training size={len(train_X)}, dev size={len(dev_X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 784)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Let’s use the following network architecture:\n",
    "\n",
    "- Two Relu-activated hidden layers, \n",
    "- Add dropout after the two hidden layers (to help the model generalize better),\n",
    "- A softmax-activated output layer. \n",
    "\n",
    "The chain combinator is like `Sequential` in PyTorch or Keras: it combines a list of layers together with a feed-forward relationship.\n",
    "\n",
    "Observe that when we **defined the model**, we don’t infor the input or the output sizes. This is inferred by the framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from thinc.api import chain, Relu, Softmax\n",
    " \n",
    "n_hidden = 32\n",
    "dropout = 0.2\n",
    "\n",
    "model = chain(\n",
    "    Relu(nO=n_hidden, dropout=dropout), \n",
    "    Relu(nO=n_hidden, dropout=dropout), \n",
    "    Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the model, we can call the `Model.initialize` method, passing in a small batch of input data `X` and a small batch of output data `Y`. This allows Thinc to infer the missing dimensions.\n",
    "\n",
    "NOTE!\n",
    "We need, however, make sure the data is on the **right device** when passing in the data. This is done by calling `model.ops.asarray` which will e.g. transform the arrays to cupy when running on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model with input dimension nI=784 and output dimension nO=10\n"
     ]
    }
   ],
   "source": [
    "# making sure the data is on the right device\n",
    "train_X = model.ops.asarray(train_X)\n",
    "train_Y = model.ops.asarray(train_Y)\n",
    "dev_X = model.ops.asarray(dev_X)\n",
    "dev_Y = model.ops.asarray(dev_Y)\n",
    "\n",
    "model.initialize(X=train_X[:5], Y=train_Y[:5])\n",
    "nI = model.get_dim(\"nI\")\n",
    "nO = model.get_dim(\"nO\")\n",
    "print(f\"Initialized model with input dimension nI={nI} and output dimension nO={nO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an optimizer\n",
    "\n",
    "Make several passes over the data, randomly selecting paired batches of the inputs and labels each time. While some machine learning libraries provide a single .fit() method to train a model all at once, Thinc puts you in charge of **shuffling and batching your data**, with the help of a few handy utility methods. model.ops.xp is an instance of either numpy or cupy, depending on whether you run the code on CPU or GPU.\n",
    "\n",
    "You might argue if this is a advantage or drawback.\n",
    "\n",
    "Let's wrap it in a training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(data, model, optimizer, n_iter, batch_size):\n",
    "    (train_X, train_Y), (dev_X, dev_Y) = data\n",
    "    indices = model.ops.xp.arange(train_X.shape[0], dtype=\"i\")\n",
    "    for i in range(n_iter):\n",
    "        batches = model.ops.multibatch(batch_size, train_X, train_Y, shuffle=True)\n",
    "        for X, Y in tqdm(batches, leave=False):\n",
    "            Yh, backprop = model.begin_update(X)\n",
    "            backprop(Yh - Y)\n",
    "            model.finish_update(optimizer)\n",
    "        # Evaluate and print progress\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X, Y in model.ops.multibatch(batch_size, dev_X, dev_Y):\n",
    "            Yh = model.predict(X)\n",
    "            correct += (Yh.argmax(axis=1) == Y.argmax(axis=1)).sum()\n",
    "            total += Yh.shape[0]\n",
    "        score = correct / total\n",
    "        print(f\" {i} {float(score):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring performance across iterations:\n",
      " 0 0.842\n",
      " 1 0.882\n",
      " 2 0.899\n",
      " 3 0.911\n",
      " 4 0.913\n",
      " 5 0.922\n",
      " 6 0.921\n",
      " 7 0.923\n",
      " 8 0.924\n",
      " 9 0.930\n",
      " 10 0.926\n",
      " 11 0.929\n",
      " 12 0.929\n",
      " 13 0.932\n",
      " 14 0.930\n",
      " 15 0.937\n",
      " 16 0.932\n",
      " 17 0.937\n",
      " 18 0.938\n",
      " 19 0.938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e066c677c644cd58a4047f1c6094e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b44d7a853664f05b7d7a4ac47d5a12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fee00c1213f4de3958123952cd4e882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358b05ff6bf64ed39719bccdb1606425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae445f2d14545fea7c9d4e802f1391c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbfd8ee138114fc28345224c5015efdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615676caf1814a85b50d1ebcafa41b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631dfcdcebc64d64b547d7cc06b64910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9592dc0646554a1fb6d19c2caf4aa309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4947b5e2818745f2b452b01736b96703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cb2aca83d04ab086d7627b3e732bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1906d3264645e99840bf86427494de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ab628d1f0e46a6b7ce33bf9c04e244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1ad5d262744b34b5c017cdfa2d778d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25af418a91b452085cb66c2c6118b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2340f6cf283346f885830584d2db7ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef465f521bf3429cbc6e2458ca7cf392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b028f49d29004bc0a83f56926738ff64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa29100a68b40a8846dfaa5464c68fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7650e5644c414278825d42f2ce5667d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from thinc.api import Adam, fix_random_seed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "fix_random_seed(0)\n",
    "optimizer = Adam(0.001)\n",
    "batch_size = 128\n",
    "print(\"Measuring performance across iterations:\")\n",
    "\n",
    "train_model(data, model, optimizer, 20, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model based on configuration\n",
    "\n",
    "Here's a config describing the model we defined above. The values in the `hyper_params` section can be referenced in other sections to keep them consistent. The `*` is used for **positional arguments** – in this case, the arguments to the `chain` function, two Relu layers and one softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hyper_params': {'n_hidden': 32, 'dropout': 0.2, 'learn_rate': 0.001},\n",
       " 'model': {'@layers': 'chain.v1',\n",
       "  '*': {'relu1': {'@layers': 'Relu.v1', 'nO': 32, 'dropout': 0.2},\n",
       "   'relu2': {'@layers': 'Relu.v1', 'nO': 32, 'dropout': 0.2},\n",
       "   'softmax': {'@layers': 'Softmax.v1'}}},\n",
       " 'optimizer': {'@optimizers': 'Adam.v1', 'learn_rate': 0.001},\n",
       " 'training': {'n_iter': 10, 'batch_size': 128}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thinc.api import Config, registry\n",
    "\n",
    "CONFIG = \"\"\"\n",
    "[hyper_params]\n",
    "n_hidden = 32\n",
    "dropout = 0.2\n",
    "learn_rate = 0.001\n",
    "\n",
    "[model]\n",
    "@layers = \"chain.v1\"\n",
    "\n",
    "[model.*.relu1]\n",
    "@layers = \"Relu.v1\"\n",
    "nO = ${hyper_params:n_hidden}\n",
    "dropout = ${hyper_params:dropout}\n",
    "\n",
    "[model.*.relu2]\n",
    "@layers = \"Relu.v1\"\n",
    "nO = ${hyper_params:n_hidden}\n",
    "dropout = ${hyper_params:dropout}\n",
    "\n",
    "[model.*.softmax]\n",
    "@layers = \"Softmax.v1\"\n",
    "\n",
    "[optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "learn_rate = ${hyper_params:learn_rate}\n",
    "\n",
    "[training]\n",
    "n_iter = 10\n",
    "batch_size = 128\n",
    "\"\"\"\n",
    "\n",
    "config = Config().from_str(CONFIG)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call `registry.make_from_config`, Thinc will first create the three layers using the specified arguments populated by the hyperparameters. It will then pass the return values (the layer objects) to `chain`. It will also create an optimizer. All other values, like the training config, will be passed through as a regular dict. \n",
    "\n",
    "If you want to change a hyperparameter or experiment with a different optimizer, all you need to change is the config. For each experiment you run, you can save a config and you'll be able to reproduce it later.\n",
    "\n",
    "Your training code can now look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6f601e1fac4f29a4effc89cd84161b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 0.850\n",
      " 1 0.876\n",
      " 2 0.904\n",
      " 3 0.910\n",
      " 4 0.916\n",
      " 5 0.919\n",
      " 6 0.922\n",
      " 7 0.924\n",
      " 8 0.927\n",
      " 9 0.926\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf5a2999e164d8b9ece1bfd47230ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaab98627a24b7797a40f6e8da7750b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4fd391623f4bc4b14db6243c2f97e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0a10ff4e4a48e190ce6c6ba8103aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27827132ad24faa91dee0ac8bd45923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4f00a86ec342b8a2ff4b7630918797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8a321219894819a21248b5513dd7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9f83259e384870bae61e36093edbcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f60186fda046e29e17208ad55f0afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_config = registry.make_from_config(config)\n",
    "\n",
    "model = loaded_config[\"model\"]\n",
    "optimizer = loaded_config[\"optimizer\"]\n",
    "n_iter = loaded_config[\"training\"][\"n_iter\"]\n",
    "batch_size = loaded_config[\"training\"][\"batch_size\"]\n",
    "\n",
    "model.initialize(X=train_X[:5], Y=train_Y[:5])\n",
    "train_model(((train_X, train_Y), (dev_X, dev_Y)), model, optimizer, n_iter, batch_size)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
