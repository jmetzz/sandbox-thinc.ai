{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600170480083",
   "display_name": "Python 3.7.7 64-bit ('thinc.ai': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: ml_datasets in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (0.1.6)\nRequirement already satisfied: tqdm>=4.41 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (4.47.0)\nRequirement already satisfied: srsly<3.0.0,>=1.0.1 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from ml_datasets) (2.0.1)\nRequirement already satisfied: catalogue<3.0.0,>=0.2.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from ml_datasets) (2.0.0)\nRequirement already satisfied: numpy>=1.7.0 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from ml_datasets) (1.18.1)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from catalogue<3.0.0,>=0.2.0->ml_datasets) (1.6.0)\nRequirement already satisfied: zipp>=0.5 in /Users/jean.metz/miniconda/envs/thinc.ai/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<3.0.0,>=0.2.0->ml_datasets) (3.1.0)\n"
    }
   ],
   "source": [
    "!pip install ml_datasets \"tqdm>=4.41\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training size=60000, test size=10000\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Fetch the dataset\n",
    "data = (train_X, train_Y), (test_X, test_Y) = tf.keras.datasets.mnist.load_data()\n",
    "# data = (train_X, train_Y), (test_X, test_Y) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "print(f\"Training size={len(train_X)}, test size={len(test_X)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Type of train_X: <class 'numpy.ndarray'>\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(60000, 28, 28)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "print(f\"Type of train_X: {type(train_X)}\")\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(60000, 28, 28, 1)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# re-shape and normalize\n",
    "# train_X = train_X.reshape(60000, 784)\n",
    "# test_X = test_X.reshape(10000, 784)\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "train_X = train_X.astype(\"float32\") / 255\n",
    "test_X = test_X.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "train_X = np.expand_dims(train_X, -1)\n",
    "test_X = np.expand_dims(test_X, -1)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "train_Y = keras.utils.to_categorical(train_Y, num_classes)\n",
    "test_Y = keras.utils.to_categorical(test_Y, num_classes)\n",
    "\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def heldout_split(train_data: Tuple, fraction=0.0)-> Tuple:\n",
    "    (train_X, train_Y) = train_data\n",
    "    if fraction <= 0:\n",
    "        return (train_X, train_Y), None\n",
    "    else:\n",
    "        total_size = len(train_X)\n",
    "        train_size = total_size - int(total_size * fraction)\n",
    "\n",
    "        # Further break training data into train / validation sets\n",
    "        (train_X, valid_X) = train_X[:train_size], train_X[train_size:] \n",
    "        (train_Y, valid_Y) = train_Y[:train_size], train_Y[train_size:]\n",
    "\n",
    "        return (train_X, train_Y), (valid_X, valid_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set, heldout_set = heldout_split((train_X, train_Y), fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train_X shape: (54000, 28, 28, 1) train_Y shape: (54000, 10)\n54000 train set\n6000 validation set\n10000 test set\n"
    }
   ],
   "source": [
    "train_X, train_Y = train_set\n",
    "heldout_X, heldout_Y = heldout_set\n",
    "\n",
    "# Print training set shape\n",
    "print(\"train_X shape:\", train_X.shape, \"train_Y shape:\", train_Y.shape)\n",
    "\n",
    "# Print the number of training, validation, and test datasets\n",
    "print(train_X.shape[0], 'train set')\n",
    "print(heldout_X.shape[0], 'validation set')\n",
    "print(test_X.shape[0], 'test set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_2 (Conv2D)            (None, 26, 26, 32)        320       \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1600)              0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 1600)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                16010     \n=================================================================\nTotal params: 34,826\nTrainable params: 34,826\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Must define the input shape in the first layer of the neural network\n",
    "# model.add(tf.keras.Input(shape=input_shape))\n",
    "# model.add(tf.keras.layers.Flatten(input_shape=(784,1)))\n",
    "# model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model:\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 54000 samples, validate on 6000 samples\nEpoch 1/5\n53952/54000 [============================>.] - ETA: 0s - loss: 0.1095 - accuracy: 0.9664\nEpoch 00001: val_loss improved from inf to 0.05402, saving model to model.weights.best.hdf5\n54000/54000 [==============================] - 32s 595us/sample - loss: 0.1094 - accuracy: 0.9664 - val_loss: 0.0540 - val_accuracy: 0.9852\nEpoch 2/5\n53888/54000 [============================>.] - ETA: 0s - loss: 0.0814 - accuracy: 0.9751\nEpoch 00002: val_loss improved from 0.05402 to 0.04324, saving model to model.weights.best.hdf5\n54000/54000 [==============================] - 32s 585us/sample - loss: 0.0813 - accuracy: 0.9751 - val_loss: 0.0432 - val_accuracy: 0.9877\nEpoch 3/5\n53952/54000 [============================>.] - ETA: 0s - loss: 0.0663 - accuracy: 0.9801\nEpoch 00003: val_loss improved from 0.04324 to 0.03947, saving model to model.weights.best.hdf5\n54000/54000 [==============================] - 32s 584us/sample - loss: 0.0663 - accuracy: 0.9801 - val_loss: 0.0395 - val_accuracy: 0.9897\nEpoch 4/5\n53888/54000 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9819\nEpoch 00004: val_loss improved from 0.03947 to 0.03400, saving model to model.weights.best.hdf5\n54000/54000 [==============================] - 32s 599us/sample - loss: 0.0571 - accuracy: 0.9819 - val_loss: 0.0340 - val_accuracy: 0.9907\nEpoch 5/5\n53952/54000 [============================>.] - ETA: 0s - loss: 0.0524 - accuracy: 0.9837\nEpoch 00005: val_loss improved from 0.03400 to 0.03389, saving model to model.weights.best.hdf5\n54000/54000 [==============================] - 32s 594us/sample - loss: 0.0525 - accuracy: 0.9837 - val_loss: 0.0339 - val_accuracy: 0.9902\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fc6b60e5350>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "import keras.callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose = 1, save_best_only=True )\n",
    "model.fit(train_X,\n",
    "         train_Y,\n",
    "         batch_size=64,\n",
    "         epochs=5,\n",
    "         validation_data=(heldout_X, heldout_Y),\n",
    "         callbacks=[checkpointer]\n",
    "         )\n",
    "         "
   ]
  }
 ]
}